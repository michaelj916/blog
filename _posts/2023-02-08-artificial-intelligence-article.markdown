---
layout: post
title:  "What is artificial intelligence (AI)?"
date:   2023-02-08 08:35:06 -0800
categories: artificial-intelligence ai nn cnn dnn science writing article
---
One of my favorite things about computer (‘s / science) is that the related fields are simultaneously one of the simplest yet most complicated fields to enter. Anyone can learn to program if they have patience. It genuinely requires patience more than intelligence; it feels like 99% of writing any piece of software is overcoming failures, persevering through maddening errors, and debugging code for hours on end.

# artificial intelligence is a buzzword
One of the most *socially* popular sub fields of computer science is artificial intelligence (AI). I see a lot of news headlines throwing this word around vaguely,  This can be really confusing, but the term “AI” is a buzzword for something called machine learning (**ML**) which is a field that encompasses things like neural networks (NN for short, these include convolutional and artificial networks denoted as CNNs and ANNs respectively), deep learning (DL), support vector machines (SVMs) and more things I am probably not mentioning here. *please comment about it if you would like*. These things I mentioned all utilize a lot of complicated mathematical concepts like linear algebra, Fourier transforms, activation functions, and complicated ass algorithms that smart people made to carry out some task like identifying a dog amongst a data set of 10,000 labeled images of dog breeds. Labeled here simply means the photo in the data set has an associated label that describes the photo.

> Let’s unpack that: 

#### For starters, what is **ML**? 
Simply put, ML is the utilization of a computerd to carry out a given task (the output) with a given data set (the input). If you think of it like a car wash, your car is the data set and the clean car is your output, and the car wash itself is a magic portal (ML) that your car goes through to become squeaky clean. Within ML there is a sub field called ANNs, which is how computer scientists have tried to simulate the neural networks that exist in the brains of biological organisms like us. ANNs are also called neural networks (NNs) or, simply, neural nets. NNs have these things (like brains) called neurons, and in NNs we connect those neurons together, each with varying weights, to produce some sort of result. Each of the neurons give information to one another, creating an extremely complex network of interconnected neurons (hence the name ‘neural network’). The connection of neurons is called an edge, and edges can be assigned a weight. When we connect these neurons together, we may have 1 or more ‘hidden layers’ in between our first step (input) and our last step (output). If we have 2 or more hidden layers, our neural network turns into a deep network. I mentioned weights earlier, what does that mean? The weights of a neural network are numbers that we can assign to the networks edges to aid the training process. Weights are not necessary for a neural network to function, but they can speed up the learning process by prioritizing certain pieces of data over others. If you think of it with the dog example briefly mentioned above, we can say this: Imagine you have a data set of 10,000 dogs. Some of the dog photos are not precise enough to be confidently called a Shih tzu or Lhasa Apso, for example. We want to express the non-precise nature of those photos compared to our other photos, so when we train our DOGNET we will give all the edges produced from the regular photos a weight of 10.0, except the non-precise ones, which we give a weight of 1.0. This will allow the more accurate photos to dominate the training process. The last thing to cover with neural networks is the training process mentioned above, which is common among all machine learning methods (recall that neural nets are just one of the many methods that exist within machine learning). So, what the hell is training? If you think about what our brains do, you will probably be dumbfounded as to how quickly (and unconsciously) we are able to perform extremely complicated tasks. For example, completing a captcha to enter a website or expressing creative thoughts or remembering information over time. Computers simply do not have this implicit natural ability to perform complicated tasks like this, so we must train them through trial and error and have them learn through their failures. To “learn through … failures” we have things called loss and optimization functions which produce floating point values that aid in the training process to improve the accuracy of our models. Within each model, we typically have an end goal in mind. The term “model” here is used to describe a machine learning process. Since every model is different, each will have a specific algorithm specialized towards whatever goal you may want to accomplish. The data of each model is usually different as well, and the ways scientists process and filter through data varies greatly. If you are curious as to how we determine what algorithms to use and what data filtering methods to use, it is generally all based off prior research in a related field. There is much more to talk about with machine learning, but this summary should suffice for a general knowledge of the field. To recap, we learned that machine learning has different sub fields like NNs, and those sub fields have sub fields too, like deep networks. For a machine learning model we need 4 key ingredients: data (labeled sets with lots of variety and clean signals), an algorithm (will vary depending on your desired goal), and loss / optimization functions (allows the model to accurately produce some result!).

